{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt JFnet to healthy vs. diseased decision on Kaggle DR data\n",
    "\n",
    "### Motivation\n",
    "\n",
    "* optretina: label noise & transfer of JFnet to new dataset with potentially differing statistics\n",
    "* kaggle dr: this should give us a feeling on how good we can get roughly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import models\n",
    "from datasets import KaggleDR\n",
    "# import plotting\n",
    "# plotting.allow_plot()\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training images\n",
    "%run ../convert_JF.py --directory=../data/kaggle_dr/train --convert_directory=../data/kaggle_dr/train_JF_512 --crop_size=512 --extension=jpeg --n_proc=4\n",
    "\n",
    "# Test images\n",
    "# TODO: I don't have the raw test images get the resized ones from vaneeda\n",
    "%run ../convert_JF.py --directory=../data/kaggle_dr/test --convert_directory=../data/kaggle_dr/test_JF_512 --crop_size=512 --extension=jpeg --n_proc=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "from Jeffrey de Fauw's network, originally trained on Kaggle DR competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(source_path=None, source_filenames=None, last_layer=None, outfile=None, batch_size=2):\n",
    "    input_var = T.tensor4('inputs')\n",
    "    weights = '../models/jeffrey_df/2015_07_17_123003_PARAMSDUMP.pkl'\n",
    "    network = models.jeffrey_df(input_var=input_var, width=512, height=512, filename=weights)\n",
    "    output_layer = network[last_layer]\n",
    "\n",
    "    feature_activations = lasagne.layers.get_output(output_layer)\n",
    "    forward_pass = theano.function([input_var], feature_activations)\n",
    "\n",
    "    dataset = KaggleDR(path_data=source_path, filename_targets=source_filenames,\n",
    "                       preprocessing=KaggleDR.jf_trafo)\n",
    "\n",
    "    n_features = np.prod(output_layer.output_shape[1:])\n",
    "    if os.path.exists(outfile):\n",
    "        print 'Feature file already exists, aborting.'\n",
    "        return\n",
    "\n",
    "    fp = np.memmap(outfile, dtype=theano.config.floatX, mode='w+',\n",
    "                   shape=(dataset.n_samples,) + output_layer.output_shape[1:]) # Each sample might still be of dim > 1\n",
    "\n",
    "    n_batches = np.ceil(dataset.n_samples/batch_size)\n",
    "\n",
    "    p = ProgressBar(n_batches)\n",
    "    \n",
    "    for i, batch in enumerate(dataset.iterate_minibatches(np.arange(dataset.n_samples), batch_size)):\n",
    "        p.animate(i)\n",
    "        inputs, _ = batch\n",
    "        fp[i*batch_size:min((i+1)*batch_size, dataset.n_samples)] = forward_pass(inputs)\n",
    "        \n",
    "    del fp # close memory mapped array\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_layer = 'conv_combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training images\n",
    "extract_features(source_path='../data/kaggle_dr/train_JF_512', \n",
    "                 source_filenames='../data/kaggle_dr/trainLabels_bin.csv', \n",
    "                 last_layer=last_layer, \n",
    "                 outfile='../data/kaggle_dr/feat_train_' + last_layer + '.npy',\n",
    "                 batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test images\n",
    "extract_features(source_path='../data/kaggle_dr/test_JF_512', \n",
    "                 source_filenames='../data/kaggle_dr/retinopathy_solution_bin.csv', \n",
    "                 last_layer=last_layer, \n",
    "                 outfile='../data/kaggle_dr/feat_test_' + last_layer + '.npy',\n",
    "                 batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier on fixed JFnet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kaggle DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_kaggle_dr_features_and_targets():\n",
    "    \n",
    "    # Training data\n",
    "    y = pd.read_csv('../data/kaggle_dr/trainLabels_bin.csv').level.values\n",
    "    nb_samples = len(y)\n",
    "    X = np.memmap('../data/kaggle_dr/feat_train_conv_combined.npy', dtype=theano.config.floatX, mode='r', \n",
    "                        shape=(nb_samples, 768))\n",
    "\n",
    "    _, nb_features = X.shape\n",
    "\n",
    "    print 'nb_samples for training:', nb_samples, 'nb_features:', nb_features\n",
    "\n",
    "    # Test data\n",
    "    df_test = pd.read_csv('../data/kaggle_dr/retinopathy_solution_bin.csv')\n",
    "    y_test = df_test.level.values\n",
    "\n",
    "    nb_samples = len(y_test)\n",
    "\n",
    "    X_test = np.memmap('../data/kaggle_dr/feat_test_conv_combined.npy', dtype=theano.config.floatX, mode='r', \n",
    "                       shape=(nb_samples, 768))\n",
    "\n",
    "    _, nb_features = X_test.shape\n",
    "\n",
    "    print 'nb_samples for testing:', nb_samples, 'nb_features:', nb_features\n",
    "    \n",
    "    return X, y, X_test, y_test, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optretina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def include_indices(exclude_path, df):\n",
    "    exclude_filenames = [fn.split('.')[0] for fn in os.listdir(exclude_path)]\n",
    "\n",
    "    images = df.filename.apply(lambda fn: fn.split('.')[0]).values\n",
    "    centre_ids = df.centre_id.values.astype(str)\n",
    "    filenames = pd.Series(['_'.join(centre_id_and_image) for\n",
    "                           centre_id_and_image in\n",
    "                           zip(centre_ids, images)])\n",
    "\n",
    "    return filenames.index[~filenames.isin(exclude_filenames)]\n",
    "\n",
    "def load_optretina_features_and_targets():\n",
    "    exclude_path = '../data/optretina/data_JF_512_exclude'\n",
    "    df = pd.read_csv('../data/optretina/OR_diseased_labels.csv')\n",
    "    include_idx = include_indices(exclude_path, df)\n",
    "    n_samples = len(df)\n",
    "    X = np.memmap('../data/optretina/feat_act_JFnet_18.npy', dtype=theano.config.floatX, mode='r', \n",
    "                       shape=(n_samples, 256*7*7))\n",
    "\n",
    "    X = X[include_idx]\n",
    "    y = df.diseased.values[include_idx]\n",
    "    n_images_total = len(df)\n",
    "    df = df.iloc[include_idx]\n",
    "    n_images_accepted = len(df)\n",
    "\n",
    "    print('Excluding {} out of {} images.'.format(n_images_total -\n",
    "                                                  n_images_accepted,\n",
    "                                                  n_images_total))\n",
    "    print('Relative class frequencies: ', pd.value_counts(y)/float(len(y)))\n",
    "    \n",
    "    X, X_test, y, y_test, df, df_test = train_test_split(X, y, df, stratify=y, test_size=0.2, random_state=1234)\n",
    "    \n",
    "    return X, y, X_test, y_test, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, X_test, y_test, df_test = load_kaggle_dr_features_and_targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition\n",
    "\n",
    "Sanity check that model has enough capacity by overfitting the training data without regularization revealed:\n",
    "* Linear classifier on top of layer 18 features (flattened) did not achieve training accuracy within a reasonable time\n",
    "* Single hidden layer with 1024 units: train_acc = 0.999 after 70 epochs, 0.9999 after 85 ep.\n",
    "* Two hidden layers (0:1024, 1:512 units): train_acc =  0.9998 after 39 ep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "l2_lambda = 0.001\n",
    "nb_features = 768  # 256*7*7\n",
    "nb_classes = 2\n",
    "\n",
    "# Two hidden layers\n",
    "mlp = Sequential()\n",
    "fc1 = Dense(1024, input_dim=nb_features, init='glorot_normal',\n",
    "                activation=LeakyReLU(alpha=0.3),\n",
    "                W_regularizer=l2(l2_lambda))\n",
    "fc1.name = 'fc1'\n",
    "mlp.add(fc1)\n",
    "fc2 = Dense(512, init='glorot_normal',\n",
    "                activation=LeakyReLU(alpha=0.3),\n",
    "                W_regularizer=l2(l2_lambda))\n",
    "fc2.name = 'fc2'\n",
    "mlp.add(fc2)\n",
    "prob = Dense(nb_classes,\n",
    "                init='glorot_normal',\n",
    "                activation='softmax',\n",
    "                W_regularizer=l2(l2_lambda))\n",
    "prob.name = 'prob'\n",
    "mlp.add(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "# optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
    "mlp.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from util import AdaptiveLearningRateScheduler\n",
    "from util import TrainingMonitor\n",
    "\n",
    "Y = np_utils.to_categorical(y)\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, stratify=y, test_size=0.2, random_state=1234)\n",
    "\n",
    "\n",
    "callbacks = []\n",
    "history = keras.callbacks.History()\n",
    "callbacks.append(history)\n",
    "callbacks.append(TrainingMonitor(history))\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               patience=10,\n",
    "                                               verbose=1,\n",
    "                                               mode='auto')\n",
    "callbacks.append(early_stopping)\n",
    "callbacks.append(keras.callbacks.ModelCheckpoint('mlp_best_weights.h5', \n",
    "                                monitor='val_loss', \n",
    "                                verbose=0, \n",
    "                                save_best_only=True, \n",
    "                                mode='auto'))\n",
    "\n",
    "\n",
    "initial_lr = float(optimizer.lr.get_value())\n",
    "callbacks.append(AdaptiveLearningRateScheduler(initial_lr=initial_lr,\n",
    "                                               decay=0.5,\n",
    "                                               patience=20,\n",
    "                                               verbose=1))\n",
    "\n",
    "cross_entropy_train  = mlp.evaluate(X_train, Y_train)\n",
    "cross_entropy_chance = - np.log(1.0/nb_classes)\n",
    "print('Cross entropy for chance level:', cross_entropy_chance)\n",
    "print('Cross entropy before training:', cross_entropy_train)\n",
    "\n",
    "mlp.fit(X_train, Y_train, nb_epoch=1000, batch_size=256,\n",
    "          class_weight={0:1, 1:1}, validation_data=(X_val, Y_val),\n",
    "          show_accuracy=True, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotting.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.load_weights('mlp_best_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posteriors = mlp.predict_proba(X_test)\n",
    "HEALTHY, DISEASED = 0, 1\n",
    "df_test.loc[:, 'prob_healthy'] = posteriors[:, HEALTHY]\n",
    "df_test.loc[:, 'prob_diseased'] = posteriors[:, DISEASED]\n",
    "df_test.loc[:,'pred'] = mlp.predict_classes(X_test)\n",
    "df_test.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, df_test.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve with DISEASED as \"positive\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "f_diseased_r, t_diseased_r, thresholds = roc_curve(y_test,\n",
    "                                                   posteriors[:, DISEASED],\n",
    "                                                   pos_label=DISEASED)\n",
    "\n",
    "roc_auc = auc(f_diseased_r, t_diseased_r, reorder=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "sns.set_context(\"talk\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(f_diseased_r, t_diseased_r,\n",
    "         label='ROC curve (auc = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.scatter([0.05], [0.8], color='g', s=50, label='recommendation British Diabetic Association')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('false diseased rate (1 - specificity)')\n",
    "plt.ylabel('true diseased rate (sensitivity)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve with DISEASED as \"positive\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, posteriors[:,\n",
    "                                                        DISEASED],\n",
    "                                              pos_label=DISEASED)\n",
    "pr_auc = auc(recall, precision, reorder=False)\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label='PR curve (auc= %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision recall curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Could we improve the performance further by fine-tuning the entire network, i.e. jfnet layers 1 to 18 together with the MLP from here?\n",
    "\n",
    "### Regarding the implementation, we have two options:\n",
    "1. Implement jfnet layers 1 to 18 in keras; that way we could use all the convenient training code from keras. Unfortunately however, keras does not have the concept of untied biases (maybe we could take the spatial mean over the biases as the std/mean is around 0.003 for all the layers)\n",
    "\n",
    "2. Export MLP obtained here as lasagne model. For now this seems to be the faster option. Vaneeda is anyway used to her training code to be used on cantor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jfnet18 = models.jfnet18_to_keras(filename='../models/jeffrey_df/2015_07_17_123003_PARAMSDUMP.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract JFnet18 features (keras implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Flatten\n",
    "jfnet18.add(Flatten())\n",
    "jfnet18.compile(optimizer='sgd', loss='binary_crossentropy')\n",
    "jfnet18.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataset = KaggleDR(path_data='../data/kaggle_dr/train_JF_512', \n",
    "                   filename_targets='../data/kaggle_dr/trainLabels_bin.csv',\n",
    "                   preprocessing=KaggleDR.jf_trafo)\n",
    "\n",
    "feature_file = '../data/kaggle_dr/feat_train_jf18_keras.npy'\n",
    "if not os.path.exists(feature_file):\n",
    "    fp = np.memmap(feature_file, dtype=theano.config.floatX, mode='w+',\n",
    "                   shape=(dataset.n_samples, 256*7*7))\n",
    "\n",
    "    n_batches = np.ceil(dataset.n_samples/batch_size)\n",
    "    p = ProgressBar(n_batches)\n",
    "\n",
    "    for i, batch in enumerate(dataset.iterate_minibatches(np.arange(dataset.n_samples), batch_size, shuffle=False)):\n",
    "        p.animate(i)\n",
    "        inputs, _ = batch\n",
    "        fp[i*batch_size:min((i+1)*batch_size, dataset.n_samples)] = jfnet18.predict(inputs, batch_size=4, verbose=0)\n",
    "\n",
    "    del fp # close memory mapped array\n",
    "else:\n",
    "    print 'Feature file already exists, aborting.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack jfnet18 and mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Flatten\n",
    "model = Sequential()\n",
    "for layer in jfnet18.layers:\n",
    "    layer.initial_weights = None # This is a current workaround; I do not understand why 'add' wants to set the weights of the previous layer?! - but I tested that model has the same weights as jfnet18 (up to incl. layer 18)\n",
    "    model.add(layer)\n",
    "    \n",
    "# Flatten output of layer 18 (256, 7, 7) to 12544 features:\n",
    "model.add(Flatten())\n",
    "\n",
    "for layer in mlp.layers:\n",
    "    model.add(layer)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "optimizer = keras.optimizers.SGD(lr=0.00001, momentum=0.9, decay=0.0, nesterov=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune jfnet18 together with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = KaggleDR(path_data='../data/kaggle_dr/train_JF_512', \n",
    "                   filename_targets='../data/kaggle_dr/trainLabels_bin.csv',\n",
    "                   preprocessing=KaggleDR.jf_trafo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = dataset.n_samples\n",
    "from sklearn.cross_validation import train_test_split\n",
    "idx_train, idx_val = train_test_split(np.arange(n_samples), test_size=0.2, stratify=dataset.y[:n_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "nb_epoch = 1000\n",
    "batch_size = 4\n",
    "nb_classes = 2\n",
    "\n",
    "from util import AdaptiveLearningRateScheduler\n",
    "from util import TrainingMonitor\n",
    "from keras.utils.generic_utils import Progbar\n",
    "\n",
    "callbacks = []\n",
    "history = keras.callbacks.History()\n",
    "callbacks.append(history)\n",
    "callbacks.append(keras.callbacks.BaseLogger())\n",
    "callbacks.append(TrainingMonitor(history))\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               patience=30,\n",
    "                                               verbose=1,\n",
    "                                               mode='auto')\n",
    "callbacks.append(early_stopping)\n",
    "\n",
    "# initial_lr = float(optimizer.lr.get_value())\n",
    "initial_lr = 1e-6\n",
    "callbacks.append(AdaptiveLearningRateScheduler(initial_lr=initial_lr,\n",
    "                                               decay=0.5,\n",
    "                                               patience=3,\n",
    "                                               verbose=1))\n",
    "\n",
    "callbacks = keras.callbacks.CallbackList(callbacks)\n",
    "\n",
    "callbacks._set_model(model)\n",
    "callbacks._set_params({\n",
    "    'batch_size': batch_size,\n",
    "    'nb_epoch': nb_epoch,\n",
    "    'nb_sample': len(idx_train),\n",
    "    'verbose': 1,\n",
    "    'do_validation': True,\n",
    "    'metrics': ['loss', 'acc', 'val_loss', 'val_acc'],\n",
    "})\n",
    "callbacks.on_train_begin()\n",
    "\n",
    "print('Train on %d samples, validate on %d samples' % (len(idx_train), len(idx_val)))\n",
    "\n",
    "model.stop_training = False\n",
    "epoch_logs = {}\n",
    "\n",
    "for epoch in range(nb_epoch):\n",
    "    callbacks.on_epoch_begin(epoch, epoch_logs)\n",
    "    # Training\n",
    "    for batch_index, (X, y) in enumerate(dataset.iterate_minibatches(idx_train, batch_size, shuffle=True)):\n",
    "        batch_logs = {}\n",
    "        batch_logs['batch'] = batch_index\n",
    "        batch_logs['size'] = len(y)\n",
    "        callbacks.on_batch_begin(batch_index, batch_logs)\n",
    "        Y = np_utils.to_categorical(y, nb_classes=nb_classes)\n",
    "        batch_logs['loss'], batch_logs['acc'] = model.train_on_batch(X, Y, accuracy=True, class_weight={0:1, 1:2.77})\n",
    "        callbacks.on_batch_end(batch_index, batch_logs)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = np.zeros((len(idx_val),1))\n",
    "    val_acc = np.zeros((len(idx_val),1))\n",
    "    progbar = Progbar(target=len(idx_val))\n",
    "    for i, (X, y) in enumerate(dataset.iterate_minibatches(idx_val, batch_size, shuffle=False)):\n",
    "        Y = np_utils.to_categorical(y, nb_classes=nb_classes)\n",
    "        vl, va = model.test_on_batch(X, Y, accuracy=True)\n",
    "        val_loss[i*batch_size:min((i+1)*batch_size, len(idx_val))] = vl\n",
    "        val_acc[i*batch_size:min((i+1)*batch_size, len(idx_val))] = va\n",
    "        progbar.update(min(i+batch_size, len(idx_val)))\n",
    "        \n",
    "    epoch_logs['val_loss'], epoch_logs['val_acc'] = np.mean(val_loss), np.mean(val_acc)\n",
    "    \n",
    "    callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "    if model.stop_training:\n",
    "        break\n",
    "\n",
    "callbacks.on_train_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('jf18mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('jf18mlp.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform predictions, iterating over the test set in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.generic_utils import Progbar\n",
    "\n",
    "test_data = KaggleDR(path_data='../data/kaggle_dr/test_JF_512', \n",
    "                   filename_targets='../data/kaggle_dr/retinopathy_solution_bin.csv',\n",
    "                   preprocessing=KaggleDR.jf_trafo)\n",
    "batch_size = 2\n",
    "print('Test on %d samples...' % (test_data.n_samples))\n",
    "\n",
    "posteriors = np.zeros((test_data.n_samples, 2))\n",
    "progbar = Progbar(target=test_data.n_samples)\n",
    "for i, (X, _) in enumerate(test_data.iterate_minibatches(np.arange(test_data.n_samples), batch_size, shuffle=False)):\n",
    "    posteriors[i*batch_size:min((i+1)*batch_size, test_data.n_samples)] = model.predict_proba(X, verbose=0)\n",
    "    progbar.update(min(i+batch_size, test_data.n_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert parameter (here, weights and biases are meant) layout of keras to be usable by lasagne\n",
    "\n",
    "lasagne needs a list of numpy arrays with alternating weights and biases: [W, b, W, b, W, b]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_values = [layer_params.get_value() for layer_params in mlp.get_params()[0]]\n",
    "[layer_param.shape for layer_param in param_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump weights to be used with lasagne later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('weights.npz', *param_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of MLP in lasagne (to be used as part of code for fine-tuning JFnet 1 to 18 together with MLP)\n",
    "\n",
    "#### Network definition\n",
    "optionally add dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DenseLayer, NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, LeakyRectify\n",
    "\n",
    "# Two hidden layer MLP in keras:\n",
    "# mlp = Sequential()\n",
    "# mlp.add(Dense(1024, input_dim=nb_features, init='glorot_normal',\n",
    "#                 activation=LeakyReLU(alpha=0.3),\n",
    "#                 W_regularizer=l2(l2_lambda)))\n",
    "# mlp.add(Dense(512, init='glorot_normal',\n",
    "#                 activation=LeakyReLU(alpha=0.3),\n",
    "#                 W_regularizer=l2(l2_lambda)))\n",
    "# mlp.add(Dense(nb_classes,\n",
    "#                 init='glorot_normal',\n",
    "#                 activation='softmax',\n",
    "#                 W_regularizer=l2(l2_lambda)))\n",
    "\n",
    "mlp = {}\n",
    "mlp['input'] = InputLayer((None, nb_features))\n",
    "mlp['fc1'] = DenseLayer(mlp['input'], num_units=1024)\n",
    "mlp['fc2'] = DenseLayer(mlp['fc1'], num_units=512, nonlinearity=LeakyRectify(leakiness=0.3))\n",
    "mlp['fc3'] = DenseLayer(mlp['fc2'], num_units=nb_classes, nonlinearity=LeakyRectify(leakiness=0.3))\n",
    "mlp['prob'] = NonlinearityLayer(mlp['fc3'], softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and set MLP weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with np.load('weights.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(mlp['prob'], param_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
