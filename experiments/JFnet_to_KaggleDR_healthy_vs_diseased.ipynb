{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapt JFnet to healthy vs. diseased decision on Kaggle DR data\n",
    "\n",
    "### Motivation\n",
    "\n",
    "* optretina: label noise & transfer of JFnet to new dataset with potentially differing statistics\n",
    "* kaggle dr: this should give us a feeling on how good we can get roughly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from progressbar import ProgressBar\n",
    "import models\n",
    "from datasets import KaggleDR\n",
    "import plotting\n",
    "plotting.allow_plot()\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training images\n",
    "%run ../convert_JF.py --directory=../data/kaggle_dr/train --convert_directory=../data/kaggle_dr/train_JF_512 --crop_size=512 --extension=jpeg --n_proc=4\n",
    "\n",
    "# Test images\n",
    "# TODO: I don't have the raw test images get the resized ones from vaneeda\n",
    "%run ../convert_JF.py --directory=../data/kaggle_dr/test --convert_directory=../data/kaggle_dr/test_JF_512 --crop_size=512 --extension=jpeg --n_proc=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "from Jeffrey de Fauw's network, originally trained on Kaggle DR competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(source_path=None, source_filenames=None, last_layer=None, outfile=None, batch_size=2):\n",
    "    input_var = T.tensor4('inputs')\n",
    "    weights = '../models/jeffrey_df/2015_07_17_123003_PARAMSDUMP.pkl'\n",
    "    network = models.jeffrey_df(input_var=input_var, width=512, height=512, filename=weights)\n",
    "    output_layer = network[last_layer]\n",
    "\n",
    "    feature_activations = lasagne.layers.get_output(output_layer)\n",
    "    forward_pass = theano.function([input_var], feature_activations)\n",
    "\n",
    "    dataset = KaggleDR(path_data=source_path, filename_targets=source_filenames,\n",
    "                       preprocessing=KaggleDR.jf_trafo)\n",
    "\n",
    "    n_features = np.prod(output_layer.output_shape[1:])\n",
    "    if os.path.exists(outfile):\n",
    "        print 'Feature file already exists, aborting.'\n",
    "        return\n",
    "\n",
    "    fp = np.memmap(outfile, dtype=theano.config.floatX, mode='w+',\n",
    "                   shape=(dataset.n_samples,) + output_layer.output_shape[1:]) # Each sample might still be of dim > 1\n",
    "\n",
    "    n_batches = np.ceil(dataset.n_samples/batch_size)\n",
    "\n",
    "    p = ProgressBar(n_batches)\n",
    "    \n",
    "    for i, batch in enumerate(dataset.iterate_minibatches(np.arange(dataset.n_samples), batch_size)):\n",
    "        p.animate(i)\n",
    "        inputs, _ = batch\n",
    "        fp[i*batch_size:min((i+1)*batch_size, dataset.n_samples)] = forward_pass(inputs)\n",
    "        \n",
    "    del fp # close memory mapped array\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_layer = '18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training images\n",
    "extract_features(source_path='../data/kaggle_dr/train_JF_512', \n",
    "                 source_filenames='../data/kaggle_dr/trainLabels_bin.csv', \n",
    "                 last_layer=last_layer, \n",
    "                 outfile='../data/kaggle_dr/feat_train_' + last_layer + '.npy',\n",
    "                 batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test images\n",
    "extract_features(source_path='../data/kaggle_dr/test_JF_512', \n",
    "                 source_filenames='../data/kaggle_dr/retinopathy_solution_bin.csv', \n",
    "                 last_layer=last_layer, \n",
    "                 outfile='../data/kaggle_dr/feat_test_' + last_layer + '.npy',\n",
    "                 batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier on fixed JFnet features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN_SHAPE = (35126, 256, 7, 7) but we flatten out trailing dimensions\n",
    "y_train = pd.read_csv('../data/kaggle_dr/trainLabels_bin.csv').level.values\n",
    "nb_samples = len(y_train)\n",
    "nb_classes = len(np.unique(y_train))\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes) # wouldn't be necessary for lasagne\n",
    "X_train = np.memmap('../data/kaggle_dr/feat_train_18.npy', dtype=theano.config.floatX, mode='r', \n",
    "                    shape=(nb_samples,256*7*7))\n",
    "\n",
    "_, nb_features = X_train.shape\n",
    "\n",
    "assert nb_features % (256*7*7) == 0, 'Assuming layer 18 features, is that wrong?'\n",
    "print 'nb_samples:', nb_samples, 'nb_features:', nb_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network definition\n",
    "\n",
    "Sanity check that model has enough capacity by overfitting the training data without regularization revealed:\n",
    "* Linear classifier on top of layer 18 features (flattened) did not achieve training accuracy within a reasonable time\n",
    "* Single hidden layer with 1024 units: train_acc = 0.999 after 70 epochs, 0.9999 after 85 ep.\n",
    "* Two hidden layers (0:1024, 1:512 units): train_acc =  0.9998 after 39 ep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "\n",
    "l2_lambda = 0.001\n",
    "\n",
    "# Two hidden layers\n",
    "mlp = Sequential()\n",
    "fc1 = Dense(1024, input_dim=nb_features, init='glorot_normal',\n",
    "                activation=LeakyReLU(alpha=0.3),\n",
    "                W_regularizer=l2(l2_lambda))\n",
    "fc1.name = 'fc1'\n",
    "mlp.add(fc1)\n",
    "fc2 = Dense(512, init='glorot_normal',\n",
    "                activation=LeakyReLU(alpha=0.3),\n",
    "                W_regularizer=l2(l2_lambda))\n",
    "fc2.name = 'fc2'\n",
    "mlp.add(fc2)\n",
    "prob = Dense(nb_classes,\n",
    "                init='glorot_normal',\n",
    "                activation='softmax',\n",
    "                W_regularizer=l2(l2_lambda))\n",
    "prob.name = 'prob'\n",
    "mlp.add(prob)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "mlp.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from util import AdaptiveLearningRateScheduler\n",
    "from util import TrainingMonitor\n",
    "\n",
    "callbacks = []\n",
    "history = keras.callbacks.History()\n",
    "callbacks.append(history)\n",
    "callbacks.append(TrainingMonitor(history))\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                               patience=30,\n",
    "                                               verbose=1,\n",
    "                                               mode='auto')\n",
    "callbacks.append(early_stopping)\n",
    "\n",
    "initial_lr = float(optimizer.lr.get_value())\n",
    "callbacks.append(AdaptiveLearningRateScheduler(initial_lr=initial_lr,\n",
    "                                               decay=0.5,\n",
    "                                               patience=25,\n",
    "                                               verbose=1))\n",
    "\n",
    "cross_entropy_train  = mlp.evaluate(X_train, Y_train)\n",
    "cross_entropy_chance = - np.log(1.0/nb_classes)\n",
    "print('Cross entropy for chance level:', cross_entropy_chance)\n",
    "print('Cross entropy before training:', cross_entropy_train)\n",
    "\n",
    "mlp.fit(X_train, Y_train, nb_epoch=1000, batch_size=256,\n",
    "          class_weight={0:1, 1:1}, validation_split=0.2,\n",
    "          show_accuracy=True, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotting.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close memory mapped training data\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST_SHAPE = (nb_samples, 256, 7, 7) but we flatten out trailing dimensions\n",
    "df_test = pd.read_csv('../data/kaggle_dr/retinopathy_solution_bin.csv')\n",
    "y_test = df_test.level.values\n",
    "\n",
    "nb_samples = len(y_test)\n",
    "\n",
    "X_test = np.memmap('../data/kaggle_dr/feat_test_18.npy', dtype=theano.config.floatX, mode='r', \n",
    "                   shape=(nb_samples, 256*7*7))\n",
    "\n",
    "_, nb_features = X_test.shape\n",
    "\n",
    "assert nb_features % (256*7*7) == 0, 'Assuming layer 18 features, is that wrong?'\n",
    "print 'nb_samples:', nb_samples, 'nb_features:', nb_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "posteriors = mlp.predict_proba(X_test)\n",
    "HEALTHY, DISEASED = 0, 1\n",
    "df_test.loc[:, 'prob_healthy'] = posteriors[:, HEALTHY]\n",
    "df_test.loc[:, 'prob_diseased'] = posteriors[:, DISEASED]\n",
    "df_test.loc[:,'pred'] = mlp.predict_classes(X_test)\n",
    "df_test.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, df_test.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve with DISEASED as \"positive\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "f_diseased_r, t_diseased_r, thresholds = roc_curve(y_test,\n",
    "                                                   posteriors[:, DISEASED],\n",
    "                                                   pos_label=DISEASED)\n",
    "\n",
    "roc_auc = auc(f_diseased_r, t_diseased_r, reorder=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(f_diseased_r, t_diseased_r,\n",
    "         label='ROC curve (auc = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('false diseased rate (1 - specificity)')\n",
    "plt.ylabel('true diseased rate (sensitivity)')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve with DISEASED as \"positive\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, posteriors[:,\n",
    "                                                        DISEASED],\n",
    "                                              pos_label=DISEASED)\n",
    "pr_auc = auc(recall, precision, reorder=False)\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label='PR curve (auc= %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision recall curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Could we improve the performance further by fine-tuning the entire network, i.e. jfnet layers 1 to 18 together with the MLP from here?\n",
    "\n",
    "### Regarding the implementation, we have two options:\n",
    "1. Implement jfnet layers 1 to 18 in keras; that way we could use all the convenient training code from keras. Unfortunately however, keras does not have the concept of untied biases (maybe we could take the spatial mean over the biases as the std/mean is around 0.003 for all the layers)\n",
    "\n",
    "2. Export MLP obtained here as lasagne model. For now this seems to be the faster option. Vaneeda is anyway used to her training code to be used on cantor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jfnet18 = models.jfnet18_to_keras(filename='../models/jeffrey_df/2015_07_17_123003_PARAMSDUMP.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack jfnet18 and mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Reshape\n",
    "model = Sequential()\n",
    "for layer in jfnet18.layers:\n",
    "    layer.initial_weights = None # This is a current workaround; I do not understand why 'add' wants to set the weights of the previous layer?!\n",
    "    model.add(layer)\n",
    "    \n",
    "# Flatten output of layer 18 (256, 7, 7) to 12544 features:\n",
    "model.add(Reshape([256*7*7]))\n",
    "\n",
    "for layer in mlp.layers:\n",
    "    model.add(layer)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune jfnet18 together with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(data_generator, samples_per_epoch, nb_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert parameter (here, weights and biases are meant) layout of keras to be usable by lasagne\n",
    "\n",
    "lasagne needs a list of numpy arrays with alternating weights and biases: [W, b, W, b, W, b]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_values = [layer_params.get_value() for layer_params in mlp.get_params()[0]]\n",
    "[layer_param.shape for layer_param in param_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump weights to be used with lasagne later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez('weights.npz', *param_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of MLP in lasagne (to be used as part of code for fine-tuning JFnet 1 to 18 together with MLP)\n",
    "\n",
    "#### Network definition\n",
    "optionally add dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DenseLayer, NonlinearityLayer\n",
    "from lasagne.nonlinearities import softmax, LeakyRectify\n",
    "\n",
    "# Two hidden layer MLP in keras:\n",
    "# mlp = Sequential()\n",
    "# mlp.add(Dense(1024, input_dim=nb_features, init='glorot_normal',\n",
    "#                 activation=LeakyReLU(alpha=0.3),\n",
    "#                 W_regularizer=l2(l2_lambda)))\n",
    "# mlp.add(Dense(512, init='glorot_normal',\n",
    "#                 activation=LeakyReLU(alpha=0.3),\n",
    "#                 W_regularizer=l2(l2_lambda)))\n",
    "# mlp.add(Dense(nb_classes,\n",
    "#                 init='glorot_normal',\n",
    "#                 activation='softmax',\n",
    "#                 W_regularizer=l2(l2_lambda)))\n",
    "\n",
    "mlp = {}\n",
    "mlp['input'] = InputLayer((None, nb_features))\n",
    "mlp['fc1'] = DenseLayer(mlp['input'], num_units=1024)\n",
    "mlp['fc2'] = DenseLayer(mlp['fc1'], num_units=512, nonlinearity=LeakyRectify(leakiness=0.3))\n",
    "mlp['fc3'] = DenseLayer(mlp['fc2'], num_units=nb_classes, nonlinearity=LeakyRectify(leakiness=0.3))\n",
    "mlp['prob'] = NonlinearityLayer(mlp['fc3'], softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and set MLP weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with np.load('weights.npz') as f:\n",
    "    param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
    "    lasagne.layers.set_all_param_values(mlp['prob'], param_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
